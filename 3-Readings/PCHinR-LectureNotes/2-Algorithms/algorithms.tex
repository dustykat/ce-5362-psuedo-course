\section{Algorithms and \textbf{R} for Computing}
An algorithm is a recipe.  A useful definition is 
\begin{quote}
An algorithm is a computational method or an ensemble of rules determining the order and form of numerical operations to be applied to a set of data $\textbf{a}(a_1,a_2,\dots)$ in order to find a new set of values $\textbf{x}(x_1,x_2,\dots)$ forming the solution of a problem.

An algorithmic procedure can be represented as 
\begin{equation}
\textbf{x}~=~f(\textbf{a})
\end{equation}

From a mathematical perspective the main concern is that the algorithm is well posed:
\begin{enumerate}
\item A solution exists for a given $\textbf{a}$.
\item The computation must lead to a single solution for $\textbf{x}$ given $\textbf{a}$.
\item The results for $\textbf{x}$ must be connected to the input $\textbf{a}$ through the Lipschitz relation.
\begin{equation}
|\delta \textbf{a}| < \eta~~\text{then}~~ |\delta \textbf{x}| < M|\delta \textbf{a}|
\end{equation}
where $M$ is a bounded natural number, $M = M(\textbf{a},\eta)$\footnote{Think of $M$ as a mapping function.}.
\end{enumerate}

Certain common problems are not well posed as stated but with reasonable assumptions can be forced into such a state.
\end{quote}

Thus an algorithm is a recipe to take input data and produce output responses through some relationships.  If a well posed problem then each result is related to the inputs, and the same inputs (in an algorithm) produce the same results.  By the recipe analogy, if you follow the same recipe each time with the same raw materials then the cake should taste the same when it is baked.

An important concept is that an algorithm operates on data (procedure-oriented); an object-oriented view is that an algorithm performs a task (generate response) based on states established by the data.  Both points of view are valid and equivalent.

Most computational hydraulics models are built (by a quirk of history) in a procedure-oriented perspective.
%Need more here

\subsection{Tools}
A practicing modeler needs a toolkit --- these tools range from the actual computation engine (EPA-SWMM, HEC-RAS, FESWMS, HSPF, WSPRO, TR-20, etc.) to analysis tools for result interpretation (\textbf{R}, \textbf{Excel}) to actual programming tools (\texttt{FORTRAN},\texttt{PERL}, etc.) to construct their own special purpose models or to test results from general purpose professional models.

In this book \textbf{R} will be used for programming, analysis, and presentation.  

\subsection{Programming}
Why programming?

There are three fundamental reasons for requiring a programming experience:
\begin{enumerate}
\item Teaching someone else a subject or procedure forces the teacher to have a reasonable understanding of the subject or procedure.  Teaching a computer (by virtue of programming) forces a very deep understanding of the underlying algorithm.  
\item You will encounter situations that general purpose programs are not designed to address; if you have a moderate ability to build your own tools when you need to, then you can.  In all likliehood, you will ``trick'' the professional program, but you cannot invent tricks unless you know a little bit about programming.
\item Programming a computer requires an algorithmic thought process --- this process is valuable in many other areas of engineering, hence the act of programming is good discipline for other problems you will encounter.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%% CHAPTER 3 %%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Variables and Operators %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\subsection{Interpolating Tabular Data -- A useful algorithm}
Material properties in physical systems are usually tabulated values.
A frequent task is to interpolate in a set of tabular values to approximate the value between rows (or columns) in the table.  
Linear interpolation is the common technique used; and the tables can are stores as either separate files, or, if the tables are small enough, they can be directly imbedded into the code.

\subsection{Linear Interpolation}
Figure \ref{fig:XYPair} is a sketch of a set of ordered pairs $(x,y)$. 

\begin{figure}[h!] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=6in]{./2-Algorithms/XYPair.jpg} 
   \caption{Sketch of two adjacent values from a table, plotted in Cartesian coordinate system.}
   \label{fig:XYPair}
\end{figure}

These pairs (there are two in the sketch) represent values in a table, for instance $x$ may represent water temperature, and $y$ may represent vapor pressure at that particular temperature.
Two adjacent values (in the table) are depicted in the sketch, and the pairs are ordered bases on the $x$-value.  

Now suppose we want to estimate the value of $y^*$ at some intermediate value $x^*$ that lies between $x_1$ and $x_2$.
As a computational task, the problem statement is to\\~\\
``Estimate the value of $y^*$ associated with the value $x^*$ given the ordered pairs $(x_1,y_1)$ and $(x_2,y_2)$.'' \\~\\
Linear interpolation simply uses the concept of similar triangles to scale the $x$ and $y$ distances between the ordered pairs to the intermediate location.   Equation \ref{eqn:InterpolationEquation} is the result of application of similar triangles to the situation described by Figure \ref{fig:LinearInterpolation} and the problem statement.

\begin{equation}
\frac{x^*-x_1}{x_2-x_1}=\frac{y^*-y_1}{y_2-y_1}
\label{eqn:InterpolationEquation}
\end{equation}

Next, apply algebra to solve Equation \ref{eqn:InterpolationEquation} for $y^*$, to obtain Equation \ref{eqn:InterpolationEquation2}

\begin{equation}
y^*=y_1+\frac{(y_2-y_1)(x^*-x_1)}{(x_2-x_1)}
\label{eqn:InterpolationEquation2}
\end{equation}

Now we can use \ref{eqn:InterpolationEquation2} to estimate values between any two data pairs.\\

\subsubsection{Interpolation of Values in Two Pairs}

Figure \ref{fig:FluidData} is a table of water properties from (CITE), that represents typically how tabular data are presented.  The temperature column is arranged in increasing order and the other properties associate with temperature across a row.

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=6in]{./2-Algorithms/FluidData.jpg} 
   \caption{Table of water properties in SI units (from CITE)}
   \label{fig:FluidData}
\end{figure}

Now suppose we wished to estimate the density of water at $44^o$ C.  
The two ordered pairs of temperature and density that surround $44^o$ C are $(40^o~C,992~kg/m^3)$ and $(50^o~C,988~kg/m^3)$.
So, to estimate the unknown density we can apply Equation \ref{eqn:InterpolationEquation2} and obtain the following result
\begin{equation}
y^*=992+\frac{(988-992)(44-40)}{(50-40)}=990.4
\label{eqn:DensityInterpolation}
\end{equation}

We might want to do this a lot, so we could write a simplistic script in R and remember to load it into our environment when we need it

\begin{lstlisting}[caption=R code demonstrating the interpolation equation, label=lst:InterpolatePairs]
# EXAMPLE # ** Interpolating Between Tabulated Pairs
interpolate2pairs<-function(xstar,x1,y1,x2,y2){
# apply interpolation equation
#   does not trap errors (divide by zero, etc)
  ystar <- y1 + (y2-y1)*(xstar-x1)/(x2-x1)
  return(ystar)
}
# In R Console
> interpolate2pairs(44,40,992,50,988)
[1] 990.4
> 
\end{lstlisting}   

For a single interrogation of a table we can stop here, but in many instances we have to interrogate a table a lot -- we want some kind of program structure to handle the work so all we have to do is pass the temperature value and have the program return the density.

\subsubsection{Interpolation of Values in Two Arrays}
To accomplish repeated interpolation we will need to have: (1) an interpolating method (we have the beginning of one above in Listing \ref{lst:InterpolatePairs}), (2) the entire table so we don't have to enter the pairs, and (3) a way to  automatically search the table so we don't have to look up values and supply them to the interpolator.

The table itself in this instance is relatively small, so we can simply assign values to some constant arrays in below in Listing \ref{lst:AssignFluidConstants}.

\begin{lstlisting}[caption=R code assigning Liquid Properties, label=lst:AssignFluidConstants]
# EXAMPLE # ** Assigning Constants
tempSI<-c(0.00,5.00,10.00,15.00,20.00,25.00,30.00,35.00,
   40.00,50.00,60.00,70.00,80.00,90.00,100.00)
densitySI<-c(1000.00,1000.00,1000.00,999.00,998.00,997.00,996.00,
   994.00,992.00,988.00,983.00,978.00,972.00,965.00,958.00)

# In R Console
> cbind(tempSI,densitySI)
      tempSI densitySI
 [1,]      0      1000
 [2,]      5      1000
 [3,]     10      1000
 [4,]     15       999
 [5,]     20       998
 [6,]     25       997
 [7,]     30       996
 [8,]     35       994
 [9,]     40       992
[10,]     50       988
[11,]     60       983
[12,]     70       978
[13,]     80       972
[14,]     90       965
[15,]    100       958
> 
\end{lstlisting}  

Returning to our example, the value $44$ lies between \texttt{tempSI[9]} and \texttt{tempSI[10]}, so we desire an algorithm that starts at \texttt{tempSI[1]} and determines if the search value is between \texttt{tempSI[1]} and \texttt{tempSI[2]}, if not, then increment the row counter and determine if the search value is between \texttt{tempSI[2]} and \texttt{tempSI[3]}, and so on.

Once we locate in the searched array where the value lies then the interpolation uses the lower and upper elements of the range to interpolate.  In the case of our example, once we determine the $44$ lies between  \texttt{tempSI[9]} and \texttt{tempSI[10]}, then the interpolation equation is

\begin{equation}
y^*=\texttt{densitySI[9]}+\frac{(\texttt{densitySI[10]}-\texttt{densitySI[9]})(44-\texttt{tempSI[9]})}{(\texttt{tempSI[10]}-\texttt{tempSI[9]})}
\label{eqn:InterpolationArrayEquation}
\end{equation}

Listing \ref{lst:GetAValue} is an \textbf{R} script that implements the search and interpolation just described.  The script assumes that the searched array ($x$) is ordered and increasing -- not a trivial assumption!  The script has some limited error handling to test if the search value actually lies in the total range of the array before beginning the search.  Once these tests are passed, then the code searches in the $x$ array for the search value $x^*$ and finds the two rows that contain the value.  Once the rows are located, the interpolation equation is used.

\begin{lstlisting}[caption=R code to Search and Interpolate, label=lst:GetAValue]
# EXAMPLE # ** Search and Interpolate
  getAvalue <- function(x,xvector,yvector){
    # returns a y value for x interpolated from (xvector,yvector)
    # xvector is assumed to be in a monotonic sequence
    # function performs limited error checks
    # NULL return is error indicator
    # T.G. Cleveland July 2007 
    #
    xvlength <- length(xvector)
    yvlength <- length(yvector)
    # check that vector lengths same
    if(xvlength != yvlength){
      message("vectors xvector and yvector different lengths -- exiting function")
      return()
    }
    # check that x in range xvector
    if(x < min(xvector)){
      message(" x too small -- exiting function")
      return()
    }
    if(x > max(xvector)){
      message(" x too big -- exiting function")
      return()
    }
    #
    for (i in 1:(xvlength-1)){
      if( (x >= xvector[i]) & (x < xvector[i+1]) ){
        result = yvector[i]+(yvector[i+1]-yvector[i])*(x - xvector[i])/
        (xvector[i+1]-xvector[i])
        return(result)
      }
      # next row  
    }
    # check if at endpoint
    if( (x >= xvector[xvlength-1]) & (x <= xvector[xvlength]) ){
      result = yvector[i]+(yvector[i+1]-yvector[i])*(x - xvector[i])/
      (xvector[i+1]-xvector[i])
      return(result)
    }
    # should never get to next line
    message("something is really wrong -- check the vectors!")
    return()
  }
 # In R Console:  
> getAvalue(44,tempSI,densitySI)
[1] 990.4
> 
  \end{lstlisting}  

Now we can load and run the \texttt{getAvalue} script and supply the two vectors plus the search value as shown in Listing \ref{lst:GetAValue}

This look-up process is readily transferred to other cases, we do have to decide if the data will be coded as constants (as was done here) or read from a file -- if the database is large the file read option is best.  In terms of building a generic look-up tool several things actually happen in a particular order.

\begin{enumerate}
\item The function call loads in the table (of reading from a file, we would have to forward declare the vectors).
\item The function searches the first vector for the bounding location of the search variable.
\item Once the boundaries are located, the interpolation is performed -- notice how the last boundary pair is handled.
\end{enumerate}

Now we can combine the data assignment, the search and interpolate into a single function so when we want to evaluate in the future we only need the single function.

Listing \ref{lst:getDensitySI} is an example of everything combined.   Here I have embedded the \texttt{getAvalue} script into the function so the whole function itself is portbable (we don't have keep track of \texttt{getAvalue}).  This embedding can be replaced with a load from a library (but then we must keep track of the path).  

The library way is preferable; if \texttt{getAvalue} needs changing, we will have to change every instance of it in the code, if we miss one the code may still run and it could be years before we discover the error because a single instance of code fragment within a larger code was missed -- its far better to only change a single instance of the function when maintenance is necessary.

\begin{lstlisting}[caption=R code to Return Water Density for Given Temperature, label=lst:getDensitySI]
# Script to return water density in SI units as a function of temperature
getDensitySI<-function(t){
# load the getAvalue() function ###################################################
  getAvalue <- function(x,xvector,yvector){
    # returns a y value for x interpolated from (xvector,yvector)
    # NULL return is error indicator
    #
    xvlength <- length(xvector)
    yvlength <- length(yvector)
    # check that vector lengths same
    if(xvlength != yvlength){
      message("vectors xvector and yvector different lengths -- exiting function")
      return()
    }
    # check that x in range xvector
    if(x < min(xvector)){
      message(" x too small -- exiting function")
      return()
    }
    if(x > max(xvector)){
      message(" x too big -- exiting function")
      return()
    }
    #
    for (i in 1:(xvlength-1)){
      if( (x >= xvector[i]) & (x < xvector[i+1]) ){
        result = yvector[i]+(yvector[i+1]-yvector[i])*(x - xvector[i])/
          (xvector[i+1]-xvector[i])
        return(result)
      }
      # next row  
    }
    # check if at endpoint
    if( (x >= xvector[xvlength-1]) & (x <= xvector[xvlength]) ){
      result = yvector[i]+(yvector[i+1]-yvector[i])*(x - xvector[i])/
        (xvector[i+1]-xvector[i])
      return(result)
    }
    # should never get to next line
    message("something is really wrong -- check the vectors!")
    return()
  }
#########################################################################################
# load the data vectors, tempSI and densitySI
tempSI<-c(0.00,5.00,10.00,15.00,20.00,25.00,30.00,35.00,
  40.00,50.00,60.00,70.00,80.00,90.00,100.00)
densitySI<-c(1000.00,1000.00,1000.00,999.00,998.00,997.00,996.00,
  994.00,992.00,988.00,983.00,978.00,972.00,965.00,958.00)
# now call getAValue
result<-getAvalue(t,tempSI,densitySI)
return(result)
}
  \end{lstlisting}  

The ``library'' approach is demonstrated in Listing \ref{lst:PlotDensity}; in this listing the path in the \texttt{source()} command is unique to my machine -- your path is likely to be different.  I find it is useful to contain all the various codes into a single directory and source that directory once to find the path, then change all the source calls to that path.  In fact that path can be a string variable and the referencing can be automatic (as long as the files exist!).

Once the look-up function is built then we can  interrogate the table many times; and even build a plot of the table -- these features are demonstrated in Listing \ref{lst:PlotDensity}.

\begin{lstlisting}[caption=R code demonstrating use of getDensitySI(), label=lst:PlotDensity]
## In R Console  
> # Example demonstrating use of functions
> # load in the functions (must exist -- use path on your machine)
> source('~/Dropbox/1-CE-TTU-Classes/UnderDevelopment/
   CE4333-PCH-R/6-RScripts/getAvalue.R')
> source('~/Dropbox/1-CE-TTU-Classes/UnderDevelopment/
   CE4333-PCH-R/6-RScripts/getDensitySI.R')
> # Now use them
> getDensitySI(44)
[1] 990.4
> getDensitySI(54)
[1] 986
> getDensitySI(88)
[1] 966.4
> t<-seq(0,100,2) # make a temperature vector 0 to 100 in 2 degree increments
> d<-numeric(0) # forward declare d to store results
> howMany<-length(t)
> for(i in 1:howMany){
+     d[i]<-getDensitySI(t[i])
+ }
> plot(t,d,type="l",xlab="Degrees Celsius",ylab="Density (kg/m^3)")
> 
\end{lstlisting}  

The resulting plot is shown on Figure \ref{fig:PlotDensity} below.
\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=5in]{./2-Algorithms/PlotDensity.jpg} 
   \caption{Plot of density versus temperature generated using the \texttt{getDensity()} function.}
   \label{fig:PlotDensity}
\end{figure}

%%%%%%%%%%%%%%%%%%%% CHAPTER 3 %%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Variables and Operators %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\subsection{Sorting}
Another frequent task in engineering hydraulics is the seemingly mundane task of sorting or ordering things. 
Here we explore a couple of simple sorting algorithms, just to show some of the thoughts that go into such a task, then will ultimately resort to the internal sorting routines built into R.
\subsubsection{Bubble Sort}
The bubble sort is a place to start despite it's relative slowness.
It is a pretty reviled algorithm (read the Wikipedia entry), but it is the algorithm that a naive programmer might cobble together in a hurry, and despite its shortcomings (its really slow and inefficient), it is robust.

Here is a description of the sorting task as described by \cite{Christian2016} (pg. 65):
\begin{quote}
``Imagine you want to alphabetize your unsorted collection of books. A natural approach would be just to scan across the shelf looking for out-of-order pairs -- Wallace followed by Pynchon, for instance -- and flipping them around. Put Pynchon ahead of Wallace, then continue your scan, looping around to the beginning of the shelf each time you reach the end. When you make a complete pass without finding any more out-of-order pairs on the entire shelf, then you know the job is done.

This process is a Bubble Sort, and it lands us in quadratic time. There are $n$ books out of order, and each scan through the shelf can move each one at most one position. (We spot a tiny problem, make a tiny fix.) So in the worst case, where the shelf is perfectly backward, at least one book will need to be moved n positions. Thus a maximum of $n$ passes through $n$ books, which gives us $O(n2)$ in the worst case.\footnote{Actually, the average running time for Bubble Sort isn't any better, as books will, on average, be $n/2$ positions away from where they?re supposed to end up. One would round the $n/2$ passes of $n$ books up to $O(n2)$.}
$\dots \dots$  For instance, it means that sorting five shelves of books will take not five times as long as sorting a single shelf, but twenty-five times as long.''
\end{quote}

Converting the word description into \textbf{R} is fairly simple.  We will have a vector of $n$ numbers (we use a vector because its easy to step through the different positions), and we will scan through the vector once (and essentially find the smallest thing), and put it into the first position.  Then we scan again from the second position and find the smallest thing remaining, and put it into the second position, and so on until the last scan which should have the remaining largest thing.  If we desire a decreasing order, simply change the sense of the comparison.  

Listing \ref{lst:MyBubbleSort.R} is an \textbf{R} script that implements the algorithm -- in the script the actual sort is treated as a function (we may actually want to use it again someday) which is loaded into the programming environment first, then an array is defined, and sorted.  The program (outside of the sorting algorithm) is really quite simple.  
\begin{itemize}
\item Load the sorting function.
\item Load contents into an array to be sorted.
\item Echo (print) the array (so we can verify the data are loaded as anticipated).
\item Sort the array, put the results back into the array (an in-place sort).
\item Report the results.
\end{itemize}

\begin{lstlisting}[caption=R code demonstrating the naive bubble sort, label=lst:MyBubbleSort.R]
##############################################################
rm(list=ls())  # clear the object list (i.e. deallocate and clear memory)
### Bubble Sort Function -- Needs to be defined before sending array to sort ###
# Bubble Sort Function
# MyLocation: ~/Dropbox/1-CE-TTU-Classes/CE4333-PCH-R/1-Lectures/Lecture03/ScriptsInLecture
# Bubble Sort with array indexing starting at [1] 
# Compare to Python or C where arrays start at [0])
# by: Theodore G. Cleveland 2017-0317
################################################################
bubble <- function(array)
{
  # Prepare the sort, need to know how many things and need a temporary store
  swap <- numeric(0) # temporary store (aka swap location)
  howMany <- length(array) # how many things to be sorted
  # The actual sorting process
  for (irow in 1:(howMany-1))
  {
    for (jrow in 1:(howMany-irow))
    {
      if( array[jrow] > array[jrow+1])
      {
        swap <- array[jrow];
        array[jrow] <- array[jrow+1];
        array[jrow+1] <- swap;
      }
    } 
  }
  # return result (sort in-place)
  return(array)
}
##############################################################

##############################################################
xarray <- c(1003,3.2,55.5,-0.0001,-6,666.6,102)  # the array to sort
print(xarray)
xarray <- bubble(xarray)
print(xarray)
##############################################################
\end{lstlisting}  

Figure \ref{fig:MyBubbleSort.jpg} is a screen capture of the script running.   
In the figure we see that the program (near the bottom of the file) assigns the values to the vector named array and the initial order of the array is $[1003,3.2,55.5,-0.0001,-6,666.6,102]$.   
The smallest value in the example is $-6$ and it appears in the 5-th position, not the 1-st as it should.  

The first pass through the array will move the largest value, $1003$, in sequence to the right until it occupies the last position.
Repeated passes through the array move the remaining largest values to the right until the array is ordered.  
One can consider the values of the array at each scan of the array as a series of transformations (\texttt{irow}-th scan) -- in practical cases we don't necessarily care about the intermediate values, but here because the size is manageable and we are trying to get our feet wet with algorithms, we can look at the values.

The sequence of results (transformations) after each pass through the array is shown in the following list:
\begin{enumerate}
\item Initial value: $[1003,3.2,55.5,-0.0001,-6,666.6,102]$.
\item First pass: $[3.2,55.5,-0.0001,-6,666.6,102,1003]$.
\item Second pass: $[3.2,-0.0001,-6,55.5,102,666.6,1003]$.
\item Third pass: $[-0.0001,-6,3.2,55.5,102,666.6,1003]$.
\item Fourth pass: $[-6,-0.0001,3.2,55.5,102,666.6,1003]$.
\item Fifth pass:  $[-6,-0.0001,3.2,55.5,102,666.6,1003]$. Sorted, fast scan.
\item Sixth pass: $[-6,-0.0001,3.2, 55.5,102,666.6,1003]$.  Sorted, fast scan.
\end{enumerate}
\begin{figure}[h!] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=6in]{./2-Algorithms/MyBubbleSort.jpg} 
   \caption{Bubble Sort implemented in \textbf{R}}
   \label{fig:MyBubbleSort.jpg}
\end{figure}

We could probably add additional code to break from the scans when we have a single pass with no exchanges -- while meaningless in this example, for larger collections of things, being able to break out when the sorting is complete is a nice feature.
\clearpage

\subsubsection{Insertion Sort}
The next type of sorting would be to select one item and locate it either left or right of an adjacent item based on its size -- like sorting a deck of cards, or perhaps a better description -- again using the bookshelf analog from \cite{Christian2016} (pg. 65):
\begin{quote}
``$\dots \dots$ You might take a different tack -- pulling all the books off the shelf and putting them back in place one by one. You'd put the first book in the middle of the shelf, then take the second and compare it to the first, inserting it either to the right or to the left. Picking up the third book, you'd run through the books on the shelf from left to right until you found the right spot to tuck it in. Repeating this process, gradually all of the books would end up sorted on the shelf and you'd be done. Computer scientists call this, appropriately enough, Insertion Sort. The good news is that it's arguably even more intuitive than Bubble Sort and doesn't have quite the bad reputation. The bad news is that it's not actually that much faster. You still have to do one insertion for each book. And each insertion still involves moving past about half the books on the shelf, on average, to find the correct place.

Although in practice Insertion Sort does run a bit faster than Bubble Sort, again we land squarely, if you will, in quadratic time. Sorting anything more than a single bookshelf is still an unwieldy prospect.''
\end{quote}

Listing \ref{lst:MyInsertionSort.R} is an \textbf{R} implementation of a straight insertion sort.  
The script is quite compact, and I used indentation and extra line spacing to keep track of the scoping delimiters.
The sort works as follows, take the an element of the array (start with 2 and work to the right) and put it into a temporary location (called \texttt{swap} in my script).  Then compare locations to the left of \texttt{swap}.  If smaller, then break from the loop, exchange values, otherwise the values are currently ordered. Repeat (starting at the next element) , when all elements have been traversed the resulting vector is sorted.  Here are the transformations for each pass through the outer loop:
\begin{enumerate}
\item Pass 0: $[1003,3.2,55.5,-0.0001,-6,666.6,102]$, Initial array.
\item Pass 1: $[3.2,1003,55.5,-0.0001,-6.,666.6,102]$.
\item Pass 2: $[3.2,55.5,1003,-0.0001,-6.,666.6,102]$.
\item Pass 3: $[-0.0001,3.2,55.5,1003,-6.,666.6,102]$.
\item Pass 4: $[-6,-0.0001,3.2,55.5,1003.,666.6,102]$.
\item Pass 5: $[-6,-0.0001,3.2,55.5,666.6,1003,102]$.
\item Pass 6: $[-6,-0.0001,3.2,55.5,102,666.6,1003]$, Sorted array.
\end{enumerate}
Figure \ref{fig:MyInsertionSort.jpg} is a screen capture of the insertion sort in operation.  
Insertion sorting is reasonably fast for small lists (about 50 or so elements) and forms the basis of the internal sorts in other routines that divide up the overall list into smaller lists, sort the smaller lists, then uses a merge to collate back to the overall list (now sorted).
\begin{lstlisting}[caption=R code demonstrating the insertion sort, label=lst:MyInsertionSort.R]
### Straight Insertion Sort Function by: Theodore G. Cleveland 2017-0317
rm(list=ls())  # clear the object list (i.e. deallocate and clear memory)
################################################################
insertSort <- function(array){
# Prepare the sort, need to know how many things and need a temporary store
  swap <- numeric(0) # temporary store (aka swap location)
  howMany <- length(array) # how many things to be sorted
  for (j in 2:howMany)  # select each position in turn
    {
    test <- 0           # set a test value, used to insert later
    swap <- array[j]    # current position to swap
    for (i in seq(j-1,1,by=-1)) #find place to insert by ...
      {
      if (array[i] <= swap)  # test if current position is bigger
        {
        test <- 1            # if true set test to 1, break inner loop
        break
        }
      array[i+1] <- array[i]  # otherwise exchange postions
      }
    if(test == 1)             # if broke from loop, insert swap
      array[i+1] <- swap      
    else
      i = 0
      array[i+1] <- swap      # otherwise swap goes to first position
    }
  return(array) }    # return result (sort in-place)
##############################################################
xarray <- c(1003,3.2,55.5,-0.0001,-6,666.6,102)  # the array to sort
print(xarray)
xarray <- insertSort(xarray)
print(xarray)
##############################################################
\end{lstlisting}  


\begin{figure}[h!] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[height=4.2in]{./2-Algorithms/MyInsertionSort.jpg} 
   \caption{Insertion Sort implemented in \textbf{R}}
   \label{fig:MyInsertionSort.jpg}
\end{figure}

\subsubsection{Merge Sort}
A practical extension of these slow sorts is called the Merge Sort.  It is an incredibly useful method.  One simply breaks up the items into smaller arrays, sorts those arrays - then merges the sub-arrays into larger arrays (now already sorted), and finally merges the last two arrays into the final, single, sorted array.

Here is a better description, again from \cite{Christian2016}:
\begin{quote}
``$\dots \dots$ information processing began in the US censuses of the nineteenth century, with the development, by Herman Hollerith and later by IBM, of physical punch-card sorting devices. In 1936, IBM began producing a line of machines called ``collators'' that could merge two separately \textbf{ordered} stacks of cards into one. \textit{As long as the two stacks were themselves sorted}, the procedure of merging them into a single sorted stack was incredibly straightforward and took linear time: simply compare the two top cards to each other, move the smaller of them to the new stack you're creating, and repeat until finished.

The program that John von Neumann wrote in 1945 to demonstrate the power of the stored-program computer took the idea of collating to its beautiful and ultimate conclusion. Sorting two cards is simple: just put the smaller one on top. And given a pair of two-card stacks, both of them sorted, you can easily collate them into an ordered stack of four. Repeating this trick a few times, you'd build bigger and bigger stacks, each one of them already sorted. Soon enough, you could collate yourself a perfectly sorted full deck -- with a final climactic merge, like a riffle shuffle's order-creating twin, producing the desired result. This approach is known today as Merge Sort, one of the legendary algorithms in computer science.''
\end{quote}

There are several other variants related to Merge Sort; Quicksort and Heapsort being relatives.  The creation of a Merge Sort is left to the reader if there is a need, and at this point we can just use the built-in \texttt{sort()}  and/or \texttt{order()} functions in \textbf{R} -- which implements either a Shellsort (useful if character strings are to be sorted) or Quicksort (used if numeric values are supplied).   We also have to supply if we want increasing or decreasing sorts.

\subsubsection{Built-In \textbf{R} Sorts}

Figure \ref{fig:BuiltInRSorts.jpg} illustrates using the built-in functions.  
For an ordinary sort, we simply use the function name \texttt{sort()} and direct its output into an object (it can even be the same vector as shown in the figure). 

If we wish to sort several related columns, based on values in one of the columns, it is easiest to construct a data frame (like a matrix), then order the contents based on one of the columns, and send the results to another data frame, or we can send the result back to itself.  Usually when we are manipulating multiple columns, we are operating in a ``relational database'' kind of mindset, and it is probably to our benefit to not destroy the original association structure.  Be aware of the syntax of a dataframe function, you will notice there is a comma that appears at the end of the function that is important for the script to function.

For example, \texttt{z <- z[order(xarray),]} will function as shown, \\ whereas \texttt{zztop <- z[order(xarray)]} will not.

\begin{figure}[h!] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=6in]{./2-Algorithms/BuiltInRSorts.jpg} 
   \caption{Sorting using built-in \textbf{R} functions}
   \label{fig:BuiltInRSorts.jpg}
\end{figure}

Now if we return to the interpolation chapter just before this one, we can immediately see a need for sorting.  The interpolation algorithm \textit{assumes} that the explanatory structure (x-axis) is ordered, otherwise the interpolation equation will return garbage.


I conclude the section on sorting with one more quoted section from \cite{Christian2016} about the value for sorting -- which is already relevant to a lot of computational hydraulics:
\begin{quote}
``The poster child for the advantages of sorting would be an Internet search engine like Google. It seems staggering to think that Google can take the search phrase you typed in and scour the entire Internet for it in less than half a second. Well, it can't -- but it doesn't need to. If you're Google, you are almost certain that (a) your data will be searched, (b) it will be searched not just once but repeatedly, and (c) the time needed to sort is somehow ``less valuable'' than the time needed to search. (Here, sorting is done by machines ahead of time, before the results are needed, and searching is done by users for whom time is of the essence.) All of these factors point in favor of tremendous up-front sorting, which is indeed what Google and its fellow search engines do.''
\end{quote}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{Exercise Set 1}
\begin{enumerate}
\item Build a function \texttt{getDensityUS()} that searches the table in Figure \ref{fig:FluidData} and returns the density of water in US customary units for a value of temperature supplied in degrees Farenheight. \\~\\
Submit your code and screen captures of the density for temperatures of $43^o~F$, $146^o~F$, and $210^o~F$.

\item Later in the class we will need functions to return viscosity to compute head losses in pipe networks.  \\
Build and test a function \texttt{getKinViscosityUS()} that searches the table in Figure \ref{fig:FluidDataUS} and returns the kinematic viscosity of water in US customary units for a value of temperature supplied in degrees Farenheight\\~\\
Submit the code and screen captures of the kinematic viscosity for temperatures of $43^o~F$, $146^o~F$, and $210^o~F$. 

\item Build and test a function \texttt{getKinViscositySI()} that searches the table in Figure \ref{fig:FluidData} and returns the kinematic viscosity of water in SI units for a value of temperature supplied in degrees Celsius \\~\\
Submit the code and screen captures of the kinematic viscosity for temperatures of $13^o~C$, $66^o~C$, and $97^o~C$.

\item Imagine you have two arrays, \texttt{array1} and \texttt{array2} that are linked in the sense that each element of \texttt{array1} is associated with the corresponding element of \texttt{array2}.  You wish to sort based on contents of \texttt{array1} and maintain the correspondence of \texttt{array2}.  In words we would simply modify the script to move and element of \texttt{array2} whenever you move an element of \texttt{array1}.

Modify the Insertion Sort script to accept two arrays, the sort is based on contents of the first array and you are to maintain correspondence with the second array.

Test your script on the following arrays:\\
\texttt{array1} = $[5,6,8,2,3,4,1]$\\
\texttt{array2} = $[24,35,63,3,8,15,0]$\\

when these are reordered, the result should be: \\
\texttt{array1} = $[1,2,3,4,5,6,8]$\\
\texttt{array2} = $[0,3,8,15,35,63]$\\

Then modify your script to read input from an ASCII file that contains the two arrays as columns (like in a spreadsheet) where you will sort on the first column, and carry along the correspondence with the second column.

Apply the script on the file \texttt{es3-pr1.txt}.

\item Repeat the exercise above but use built-in \textbf{R} functions.\footnote{My solution uses the \texttt{data.frame()} and \texttt{order()} functions.  There are probably several other ways to accomplish the goal -- corresponding sorts are hugely important in many practical situations.} 
\end{enumerate}

This exercise set is also located on the class server as \texttt{ES-1}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% need more here
